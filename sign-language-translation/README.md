The objective of this project is to develop a real-time British Sign Language (BSL) translation system that generates both text and speech outputs, bridging communication gaps for the deaf and hard-of-hearing community. The system processes video input through multiple stages, including frame extraction, facial landmark detection, and spatial feature learning using Convolutional Neural Networks (CNNs). Temporal gesture patterns are modeled using Recurrent Neural Networks (RNNs), while emotional context is classified using a Multi-Layer Perceptron (MLP)-based emotion recognition module.

The final translated speech and emotion-aware audio are synthesized using advanced neural speech-generation models such as Tacotron 2 and WaveGlow. Unlike traditional systems that focus solely on gesture recognition, this approach integrates emotional context, resulting in expressive, natural, and contextually rich communication. The system demonstrates improved accuracy, enhanced emotional realism, and real-time performance, making it suitable for inclusive applications across education, healthcare, and public-service environments.
